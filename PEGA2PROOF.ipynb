{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-317-c48d5a0c7903>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;31m#main(\"proof2.pega\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-317-c48d5a0c7903>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpega\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mlines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpega\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '-f'"
     ]
    }
   ],
   "source": [
    "#Viktor Rumanuk\n",
    "#vrumanuk@gmail.com\n",
    "\n",
    "import sys\n",
    "\n",
    "#Remove doublecuts\n",
    "#Ideally this is not needed but the proof checker does some\n",
    "#automatic consolidation of negatives and I wanted to avoid corner cases due to both programs conflicting\n",
    "def doublecut_reduce(term):\n",
    "    prev = []\n",
    "    nest = 0\n",
    "    #Simply step through every character in the string, keeping track of depth, and remove any double sets of parentheses\n",
    "    for n, c in enumerate(term):\n",
    "        if c == '(':\n",
    "            prev.append(n)\n",
    "            nest += 1\n",
    "        elif c == ')':\n",
    "            nest -= 1\n",
    "            if prev[-1] > 0 and term[prev[-1]-1] == '(' and n < (len(term)-1) and term[n+1] == ')':\n",
    "                if nest == 1:\n",
    "                    term = term[:prev[-1]-1] + '[' + term[prev[-1]+1:n] + ']' + term[n+2:]\n",
    "                else:\n",
    "                    term = term[:prev[-1]-1] + term[prev[-1]+1:n] + term[n+2:]\n",
    "                term = doublecut_reduce(term)\n",
    "                break\n",
    "            prev = prev[:-1]\n",
    "    \n",
    "    return term\n",
    "\n",
    "#Splits a PEGA string into separate terms\n",
    "#Each term returned corresponds to a separate top-level cut of data in the EG proof\n",
    "def separate_parens(term):\n",
    "    seps = [0]\n",
    "    nest = 0\n",
    "    #Very similar to DoubleCutReduce: We just walk through the string and track when we \"resurface\" to depth 0\n",
    "    for n, c in enumerate(term):\n",
    "        if c == '(' and nest == 0 and n != 0:\n",
    "            seps.append(n)\n",
    "            nest += 1\n",
    "        elif c == '(':\n",
    "            nest += 1\n",
    "        elif c == ')' and nest == 1:\n",
    "            nest = 0\n",
    "            seps.append(n+1)\n",
    "        elif c == ')':\n",
    "            nest -= 1\n",
    "\n",
    "    fseps = []\n",
    "    for n in range(len(seps)-1):\n",
    "        if seps[n] != seps[n+1]:\n",
    "            fseps.append(seps[n])\n",
    "    \n",
    "    fseps.append(seps[-1])\n",
    "        \n",
    "    split = [term[fseps[i]:fseps[i+1]] for i in range(len(fseps)-1)]\n",
    "    if fseps[-1] != len(term):\n",
    "        split.append(term[fseps[-1]:])\n",
    "    return split\n",
    "\n",
    "#Helper function that removes extra identifiers I used during double cut reduction\n",
    "#These existed to prevent the program from \"losing\" a term from one step of the proof to the next due to new variables\n",
    "#appearing on the canvas as the result of a double cut reduction\n",
    "def brace_split(line):\n",
    "    final_line = []\n",
    "    \n",
    "    for t in line:\n",
    "        nt = \"\"\n",
    "        l = t.find('[')\n",
    "        r = t.find(']')\n",
    "        \n",
    "        if l != -1 and r != -1:\n",
    "            final_line.append(t[:l])\n",
    "            final_line.append(t[l+1:r])\n",
    "            final_line.append(t[r+1:])\n",
    "        else:\n",
    "            nt = t.replace('[', '')\n",
    "            nt = nt.replace(']', '')\n",
    "        \n",
    "            if len(nt) > 0:\n",
    "                final_line.append(nt)\n",
    "    \n",
    "    final_line = [x for x in final_line if len(x) > 0]\n",
    "    return final_line\n",
    "                \n",
    "\n",
    "#Explicitly apply negation and insert correct connective between variables based on depth\n",
    "def apply_cut(term):\n",
    "    depth = 0\n",
    "    nt = \"\"\n",
    "    n = 0\n",
    "    while n < len(term):\n",
    "        c = term[n]\n",
    "        if c == '(':\n",
    "            depth += 1\n",
    "            if term[n+3] != ')':\n",
    "                nt += c\n",
    "        elif c == ')':\n",
    "            depth -=1\n",
    "            if term[n-3] != '(':\n",
    "                nt += c\n",
    "            if n < len(term)-1 and term[n+1] != ')':\n",
    "                if depth % 2 == 0:\n",
    "                    nt += \" & \"\n",
    "                else:\n",
    "                    nt += \" | \"\n",
    "        elif c == '|':\n",
    "            if n < len(term)-1 and term[n+1] != ')':\n",
    "                if depth % 2 == 0:\n",
    "                    nt += \" & \"\n",
    "                else:\n",
    "                    nt += \" | \"\n",
    "        else:\n",
    "            if depth % 2 == 1:\n",
    "                nt += \"!\"\n",
    "            \n",
    "            nt += c\n",
    "        \n",
    "        n += 1\n",
    "    \n",
    "    if nt[0] == '(' and nt[-1] == ')':\n",
    "        return nt[1:-1]\n",
    "    \n",
    "    return nt\n",
    "\n",
    "#Returns a list of tuples where each tuple describes the alteration that occurred\n",
    "#on the changing term for each line in the proof\n",
    "def get_deltas(lines):\n",
    "    return [(list(set(lines[n]) - set(lines[n+1]))[0], list(set(lines[n+1]) - set(lines[n]))[0]) for n in range(len(lines)-1)]\n",
    "\n",
    "#Find the line in the proof that we need to reference for the current proof step\n",
    "def find_line(ts, targ):\n",
    "    #Ignore double negatives\n",
    "    if targ[0] == '!' and targ[1] == '!':\n",
    "        targ = targ[2:]\n",
    "    \n",
    "    for n, c in enumerate(ts):\n",
    "        if c == targ:\n",
    "            return n+1\n",
    "    \n",
    "    print \"FAILED TO FIND REFERENCE\"\n",
    "\n",
    "#Generates the proof!\n",
    "def get_prooflines(ts, blob, premisecount):\n",
    "    prooflines = []\n",
    "    \n",
    "    for d in blob:\n",
    "        delt = d[0]\n",
    "        op = d[1]\n",
    "        pl = \"\"\n",
    "        \n",
    "        #Line generation is based on which operation took place in the EG proof\n",
    "        if op == \"ER:\":\n",
    "            #Erasure is simply a 'simplification' operation wherein a conjuction is removed from\n",
    "            pl = \"lin \" + delt[1] + \":Simplification: \" + str(find_line(ts, delt[0]))\n",
    "        elif op == \"IN\":\n",
    "            #Insertion is simply an 'addition' operation wherein a disjunction is added to\n",
    "            pl = \"lin \" + delt[1] + \":Addition: \" + str(find_line(ts, delt[0]))\n",
    "        else:\n",
    "            #Iteration! We use either Modus Ponens or Modus Tollens\n",
    "            ponens = True\n",
    "            if len(delt[0]) > len(delt[1]):\n",
    "                if delt[0].find(delt[1]) == 0:\n",
    "                    ponens = False\n",
    "                diff = delt[0].replace(delt[1], \"\")\n",
    "            else:\n",
    "                diff = delt[1].replace(delt[0], \"\")\n",
    "\n",
    "            diff = diff.replace(\" | \", \"\")\n",
    "            diff = diff.replace(\" & \", \"\")\n",
    "            diff = diff.replace(\"()\", \"\")\n",
    "            \n",
    "            #Decide between Modus Ponens and Modus Tollens based on\n",
    "            #whether the beginning or the end of the disjunction was removed during the relevant EG proof step\n",
    "            #Ponens vs Tollens just decides how to structure the implication we construct\n",
    "            #I did it this way because it is extremely tedious to justify to Mike Usher's program\n",
    "            #that (A or B) or C == (C or B) or A\n",
    "            if ponens:\n",
    "                implication = \"!\" + diff + \">(\" + delt[1].replace(\" \", \"\") + \")\"\n",
    "                pl = \"lin \" + implication + \":Implication: \" + str(find_line(ts, delt[0]))\n",
    "                ts.append(implication)\n",
    "                prooflines.append(pl)\n",
    "                pl = \"lin \" + delt[1].replace(\" \", \"\") + \":Modus Ponens: \" + str(find_line(ts, \"!\" + diff)) + \" \" + str(len(prooflines)+premisecount)\n",
    "            else:\n",
    "                implication = \"!(\" + delt[1].replace(\" \", \"\") + \")>\" + diff\n",
    "                pl = \"lin \" + implication + \":Implication: \" + str(find_line(ts, delt[0]))\n",
    "                ts.append(implication)\n",
    "                prooflines.append(pl)\n",
    "                pl = \"lin \" + delt[1].replace(\" \", \"\") + \":Modus Tollens: \" + str(find_line(ts, \"!\" + diff)) + \" \" + str(len(prooflines)+premisecount)\n",
    "        \n",
    "        #Make sure to update list of current knowns\n",
    "        ts.append(delt[1])\n",
    "        prooflines.append(pl)\n",
    "    \n",
    "    return prooflines\n",
    "\n",
    "def main(path):\n",
    "    with open (path, \"r\") as pega:\n",
    "        lines=pega.read().split(\"\\n\")[:-1]\n",
    "    \n",
    "    #Pull in PEGA data line by line and format away unnecessary characters\n",
    "    lines = [''.join(c for c in line if c not in '[]-./').upper() for line in lines]\n",
    "    lines = [x for x in lines if x != '\\r']\n",
    "    if lines[0][-1] == '\\r':\n",
    "        lines = [x[:-1] for x in lines]\n",
    "    \n",
    "    #Store the operator portion of each line for later use\n",
    "    operators = [x[:3] for x in lines]\n",
    "    \n",
    "    #Split the raw PEGA data into our desired format\n",
    "    lines = [brace_split(separate_parens(doublecut_reduce(x[3:]))) for x in lines]\n",
    "    \n",
    "    #Remove cuts and replace with appropriate negation of term\n",
    "    processed_lines = [[apply_cut(y) for y in x] for x in lines]\n",
    "    line_nums = [0] + [n for n in range(1, len(processed_lines)) if processed_lines[n] != processed_lines[n-1]]\n",
    "    processed_lines = [processed_lines[n] for n in range(len(processed_lines)) if n in line_nums]\n",
    "    operators = [operators[n] for n in range(len(operators)) if n in line_nums]\n",
    "\n",
    "    #Retrieve the changes that occurred between each line in the graph\n",
    "    deltas = get_deltas(processed_lines)\n",
    "    \n",
    "    #Signify premises with the \"pre\" prefix\n",
    "    premises = [\"pre \" + x.replace(\" \", \"\") for x in list(set(processed_lines[0]))]\n",
    "    \n",
    "    #Store premises as knowns for use in generating the proof lines\n",
    "    truths = list(set(processed_lines[0]))\n",
    "\n",
    "    #Generate the lines of the proof\n",
    "    proof_lines = get_prooflines(truths, zip(deltas, operators), len(premises))\n",
    "    \n",
    "    #Print the completed proof. This output can be fed into Mike Usher's\n",
    "    #proof checker for verification of the original EG proof\n",
    "    for p in premises:\n",
    "        print p\n",
    "    \n",
    "    for l in proof_lines:\n",
    "        print l\n",
    "\n",
    "main(\"proof2.pega\")\n",
    "#if __name__ == \"__main__\":\n",
    "#    main(sys.argv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
